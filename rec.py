import argparse, json, time
from collections import deque
from innertube import InnerTube
import pandas as pd
from tqdm import tqdm

# Remember to update innertube as said in README.md
client = InnerTube("WEB")

def get_recs(video_id, limit=11):
    """
    Function that uses innertube API to get recommendations. To nail down where the recommendations are stored in API result we use notebook for exploration.

    You might have to change this part regularly if innertube changes.
    """

    try:
        resp = client.next(video_id=video_id)
        results = (
            resp["contents"]["twoColumnWatchNextResults"]["secondaryResults"]["secondaryResults"]["results"]
        )
        ids = [
            item['lockupViewModel']['contentId']
            for item in results
            if "lockupViewModel" in item and len(item['lockupViewModel']['contentId']) == 11
        ]
        print(f"Collected {len(ids)} ids from {video_id}")
        return ids[:limit]
    except Exception as e:
        print(f"[warn] {video_id}: {e}")
        return []

def recursive_dfs(current_id: str, current_depth: int, max_depth: int):
    """
    Trees are generated by DFS instead of BFS (previous version). Saving time from construction of tree and cycle-detection.

    This function should hopefully stay as is most of the time.
    """

    if current_depth == max_depth:
        return dict(), 0
    
    sub_tree = dict()
    stack = get_recs(current_id)
    l = len(stack)
    for child_id in stack:
        child_tree, ls = recursive_dfs(child_id, current_depth+1, max_depth)
        l += ls
        sub_tree[child_id] = child_tree

    return sub_tree, l

def crawl(start_id: str, depth: int = 2):
    """
    DFS Crawl, wrapper.
    """
    tree = dict()
    sub_tree, l = recursive_dfs(start_id, 0, depth)
    tree [start_id] = sub_tree
    return tree, l


if __name__ == "__main__":
    ap = argparse.ArgumentParser()
    ap.add_argument("--input", help="CSV for interested videos. Each row will result in a separate _rec file.")
    ap.add_argument("--depth", type=int, default=3, help="Max depth for recommendation tree, will result in limit^depth points.")
    ap.add_argument("--max_retries", type=int, default=5, help="Number of times to retry recommendation scrape per video id.")
    # ^ For some reason I have not figured out yet, innertube sometimes does not reply with a result, so we try X times again to get a correct result
    # This might be a rate-limit thing, but unlikely
    args = ap.parse_args()

    input_file = args.input
    depth = args.depth
    max_retries = args.max_retries

    df = pd.read_csv(input_file, header=0)
    ids = df["id"].to_list()
    print(f"{len(ids)} IDs to process")

    for id in ids:
        print(f"===> Crawling {id}.")
        out_file = f"out/{id}_rec.json" # Hard-coded out/ if you really need to change it you can add as argument for flexibility

        # This is the redo logic to combat innertube shenanigans I described
        retries = 0
        tree, number_nodes = crawl(id, depth)
        while number_nodes < 10 and retries < max_retries: # notice check for number of nodes captured
            print("---> Sleeping to try again")
            time.sleep(2)
            tree, number_nodes = crawl(id, depth)
            retries += 1

        with open(out_file, "w") as fh:
            json.dump(tree, fh, indent=2)
        print(f"===> Total {number_nodes} videos captured.")
        # ^ This number will not be always exactly limit^depth, because some videos become ads, or unreachable, etc, so as long as it's reasonably close I say no worries
